# Claude Code Analysis - Arbeitsrichtlinien

## ‚ö†Ô∏è KRITISCHE REGEL: FAKTEN VOR ANNAHMEN

**Problem:** KI-Assistenten neigen dazu, Annahmen zu treffen ohne diese zu verifizieren.

**Konsequenz:** Falsche Reviews, ung√ºltige TODO-Listen, verschwendete Zeit.

---

## üî¥ VERBINDLICHE ARBEITSWEISE F√úR CODE-REVIEWS

### 1. BUILD FIRST - ANALYSE SECOND

**IMMER in dieser Reihenfolge:**

```bash
# SCHRITT 1: Build-Status pr√ºfen (PFLICHT)
./mvnw clean compile
./mvnw test

# SCHRITT 2: Build-Ergebnis dokumentieren
# - Kompiliert? JA/NEIN
# - Tests laufen? JA/NEIN
# - Fehler? Welche genau?

# SCHRITT 3: Nur bei tats√§chlichen Fehlern analysieren
```

**‚ùå NIEMALS:**
- Code lesen ‚Üí Annahmen treffen ‚Üí Review schreiben
- Imports sehen ‚Üí "k√∂nnte fehlen" ‚Üí als Problem listen
- TODO-Kommentare ‚Üí "ist nicht implementiert" ‚Üí als Fehler werten

**‚úÖ IMMER:**
- Build ausf√ºhren ‚Üí Fakten sammeln ‚Üí Review auf Basis von Fakten

---

### 2. TEST-STATUS ERMITTELN

**KRITISCH:** Maven-Logs sind NICHT die Quelle der Wahrheit f√ºr Test-Status!

**‚ùå NIEMALS:**
- Maven-Log-Ausgaben als Test-Failures interpretieren
- "ERROR" in Logs = Test fehlgeschlagen annehmen
- Build-Log-Text parsen um Test-Status zu ermitteln

**‚úÖ IMMER:**
- Surefire XML-Reports lesen (`target/surefire-reports/TEST-*.xml`)
- Attribute `tests`, `errors`, `failures` auswerten
- Nur XML-Attribute sind verl√§sslich

**Warum?**
- Maven-Logs enthalten ERROR-Ausgaben auch bei erfolgreichen Tests
- Tests k√∂nnen bewusst Fehler-Szenarien testen (z.B. FlywayMigrationErrorTest)
- Log-Output ist f√ºr Menschen, XML-Reports f√ºr Maschinen

**Korrekte Methode:**

```bash
# Test-Status ermitteln
./mvnw clean test

# Reports analysieren (nicht Logs!)
cd target/surefire-reports
grep '<testsuite' TEST-*.xml
```

**XML-Interpretation:**
```xml
<!-- TEST PASSED -->
<testsuite tests="1" errors="0" failures="0" skipped="0">

<!-- TEST FAILED -->
<testsuite tests="1" errors="1" failures="0" skipped="0">  ‚Üê ERROR
<testsuite tests="1" errors="0" failures="1" skipped="0">  ‚Üê FAILURE
```

**Beispiel - FALSCH:**
```
Maven Log zeigt: "[ERROR] Tests run: 10, Failures: 1, Errors: 7"
Annahme: 8 Tests sind fehlgeschlagen
Review: "‚ùå 8 von 10 Tests schlagen fehl"
```

**Beispiel - RICHTIG:**
```
Maven Log zeigt: "[ERROR] Tests run: 10, Failures: 1, Errors: 7"
Aktion: Surefire XML-Reports lesen
Ergebnis: Alle 10 Tests haben errors="0" failures="0"
Review: "‚úÖ Alle 10 Tests bestehen"
Anmerkung: ERROR in Logs sind Teil der Tests selbst
```

---

### 3. DATEI-EXISTENZ VERIFIZIEREN

**Regel:** Jede Behauptung √ºber fehlende Dateien MUSS verifiziert werden.

```bash
# Bevor Du schreibst: "Klasse X fehlt"
find . -name "ClassName.java"
# oder
find . -type f -name "*.java" | grep ClassName

# Bevor Du schreibst: "Datei Y existiert nicht"
ls -la path/to/file.ext
```

**Beispiel - FALSCH:**
```
Gesehen: import com.example.MissingClass;
Annahme: Klasse fehlt
Review: "‚ùå MissingClass.java fehlt, bitte implementieren"
```

**Beispiel - RICHTIG:**
```
Gesehen: import com.example.MissingClass;
Aktion: find . -name "MissingClass.java"
Ergebnis: ./src/main/java/com/example/MissingClass.java gefunden
Review: [nichts, Klasse existiert]
```

---

### 3. BEWEISE F√úR JEDE BEHAUPTUNG

**Jede Schwachstelle im Review MUSS haben:**

1. ‚úÖ **Dateipfad** mit Zeilennummer
2. ‚úÖ **Konkreter Code-Auszug** als Beweis
3. ‚úÖ **Reproduzierbares Problem** (Build-Fehler, Test-Failure, etc.)
4. ‚úÖ **Impact-Beschreibung** (Was ist die Konsequenz?)

**Beispiel - UNZUREICHEND:**
```markdown
## Problem: Fehlende Exception-Behandlung
Die Exception-Behandlung ist nicht gut.
```

**Beispiel - KORREKT:**
```markdown
## Problem: Zu breite Exception-Catches

**Datei:** `FlywayDeploymentProcessor.java:75-86`

**Code:**
\`\`\`java
} catch (Exception e) {  // ‚Üê Zu breit
    if (hasMigrations(deploymentUnit)) {
        throw new DeploymentUnitProcessingException(...);
    }
}
\`\`\`

**Problem:**
- F√§ngt alle Exceptions (SQLException, NamingException, etc.)
- Macht Debugging schwierig
- Versteckt unerwartete Fehler

**Impact:** Produktions-Fehler k√∂nnten unbemerkt bleiben

**Nachweis:** Keine Build-Fehler, aber Code-Smell (SonarQube Rule: S1181)

**L√∂sung:** Spezifische Exception-Typen catchen
```

---

### 4. CHECKLISTE F√úR REVIEWS

**VOR dem Schreiben eines Reviews:**

- [ ] Build ausgef√ºhrt? (`./mvnw clean install`)
- [ ] Tests gelaufen? (`./mvnw test`)
- [ ] **Surefire-Reports gelesen?** (`target/surefire-reports/TEST-*.xml`)
- [ ] Alle "fehlenden" Dateien per `find` gesucht?
- [ ] Jedes Problem mit Dateipfad + Zeilennummer dokumentiert?
- [ ] Jedes Problem ist reproduzierbar?
- [ ] Keine Annahmen ohne Beweise?
- [ ] Build-Status klar dokumentiert? (‚úÖ/‚ùå)
- [ ] **Test-Status aus XML-Reports** (nicht aus Logs!)

**Wenn auch nur EIN Punkt mit "Nein" beantwortet wird:**
‚Üí **STOP** ‚Üí Review ist unvollst√§ndig ‚Üí Nacharbeiten!

**WICHTIG: Bei vermeintlichen Problemen:**
- ‚ùå NIEMALS als Fakt pr√§sentieren
- ‚úÖ IMMER beim User best√§tigen lassen
- Formulierung: "Ich sehe X - k√∂nnen Sie das best√§tigen?"
- Erst nach Best√§tigung ins Review aufnehmen

---

### 5. KORREKTE PROBLEM-KATEGORISIERUNG

**Kritisch (P0) - Blockiert Build:**
- ‚ùå Kompilierfehler (fehlendes `;`, Syntax-Fehler)
- ‚ùå Fehlende Dependencies (nicht in pom.xml)
- ‚ùå Test-Failures die Build brechen
- ‚ùå **NACHWEIS ERFORDERLICH:** Build-Log mit Fehler

**Hoch (P1) - Funktioniert, aber problematisch:**
- ‚ö†Ô∏è Security-Issues (ExampleDS in Production)
- ‚ö†Ô∏è Veraltete Dependencies mit bekannten CVEs
- ‚ö†Ô∏è Dokumentierte Features nicht implementiert
- ‚ö†Ô∏è **NACHWEIS ERFORDERLICH:** Code-Stelle + Begr√ºndung

**Mittel (P2) - Verbesserungsw√ºrdig:**
- üí° Code-Smells (zu breite Catches)
- üí° Performance-Verbesserungen
- üí° Test-Coverage-Gaps
- üí° **NACHWEIS ERFORDERLICH:** Metric oder Code-Beispiel

**Niedrig (P3) - Nice to have:**
- üìù Dokumentations-Verbesserungen
- üìù Refactoring-M√∂glichkeiten
- üìù **NACHWEIS ERFORDERLICH:** Begr√ºndung warum es besser w√§re

---

### 6. ANTI-PATTERNS ZU VERMEIDEN

**‚ùå NIEMALS MACHEN:**

```markdown
## Problem: Klasse X fehlt
Die Klasse X wird importiert aber existiert nicht.
```
‚Üí **Warum falsch?** Keine Verifikation durchgef√ºhrt!

**‚ùå NIEMALS MACHEN:**
```markdown
## Problem: Tests fehlen
Es gibt keine Tests f√ºr Feature Y.
```
‚Üí **Warum falsch?** Nicht nachgepr√ºft! (Tests k√∂nnten existieren)

**‚ùå NIEMALS MACHEN:**
```markdown
## Problem: Build bricht
Der Code kompiliert wahrscheinlich nicht.
```
‚Üí **Warum falsch?** "Wahrscheinlich" ist keine Fakten-Basis!

---

### 7. KORREKTUR-PROTOKOLL

**Wenn ein Fehler passiert:**

1. ‚úÖ Fehler sofort eingestehen
2. ‚úÖ Root Cause analysieren
3. ‚úÖ Korrekte Information nachliefern
4. ‚úÖ In CLAUDE.MD dokumentieren (diese Datei!)
5. ‚úÖ Beim n√§chsten Review die Lektion anwenden

**Beispiel:**
```markdown
## Fehler vom 2025-10-02

### Fehler 1: Fehlende Klassen behauptet
**Was:** Behauptet, 3 Klassen fehlen
**Realit√§t:** Alle 3 Klassen existierten
**Root Cause:** Keine `find`-Suche durchgef√ºhrt vor Behauptung
**Lektion:** IMMER Dateisystem pr√ºfen vor "fehlt"-Aussagen

### Fehler 2: Test-Failures aus Logs interpretiert
**Was:** Behauptet, 8 von 10 Tests schlagen fehl
**Realit√§t:** Alle 10 Tests bestehen (laut Surefire-Reports)
**Root Cause:** Maven-Logs interpretiert statt XML-Reports gelesen
**Lektion:** NIEMALS Logs parsen - IMMER Surefire XML-Reports lesen

**Fix:** Diese Richtlinie erweitert (CLAUDE.MD)
```

---

## üìã REVIEW-TEMPLATE

Verwende dieses Template f√ºr jeden Review:

```markdown
# Code Review: [Projektname]

## Build-Status
- Kompilierung: ‚úÖ/‚ùå
- Unit-Tests: ‚úÖ/‚ùå (X passed, Y failed)
- Integration-Tests: ‚úÖ/‚ùå (X passed, Y failed)
- **Beweis:** Surefire XML-Reports (nicht Maven-Logs!)
- **Befehl:** `./mvnw clean test` ausgef√ºhrt am [Datum]

## Verifizierte Schwachstellen

### [Problem-Titel]
- **Datei:** `path/to/file.java:123`
- **Code:** [Konkreter Code-Auszug]
- **Problem:** [Was ist falsch?]
- **Impact:** [Welche Konsequenz?]
- **Nachweis:** [Build-Log, Test-Failure, oder Tool-Output]
- **Priorit√§t:** P0/P1/P2/P3
- **Aufwand:** X Stunden

[... weitere Probleme ...]

## TODO-Liste

### Kritisch (P0)
[Nur Probleme die Build brechen]

### Hoch (P1)
[Funktioniert, aber problematisch]

### Mittel (P2)
[Verbesserungen]

### Niedrig (P3)
[Nice to have]

## Nicht-Probleme (verifiziert)

[Dinge die NICHT Probleme sind, aber gepr√ºft wurden]
- ‚úÖ Alle Klassen existieren (verifiziert via `find`)
- ‚úÖ Build l√§uft durch (verifiziert via `mvnw clean install`)
- ‚úÖ Tests bestehen (verifiziert via `mvnw test`)
```

---

## üéØ ZUSAMMENFASSUNG

**Die goldene Regel:**

> **NIEMALS eine Behauptung ohne Beweis.**
>
> Jede Zeile im Review muss entweder:
> 1. Mit einem Dateipfad + Zeilennummer belegt sein, ODER
> 2. Mit einem Build-/Test-Output belegt sein, ODER
> 3. Mit einem Tool-Output belegt sein (SonarQube, etc.)
>
> Wenn Du keine Beweise hast: Nicht ins Review schreiben!

**Die drei Schritte:**
1. **Build** ‚Üí Fakten sammeln
2. **Verify** ‚Üí Behauptungen pr√ºfen
3. **Document** ‚Üí Nur Fakten schreiben

**Konsequenz bei Nicht-Einhaltung:**
- Vertrauensverlust
- Verschwendete Entwickler-Zeit
- Falsche Priorisierungen
- Nutzloser Review

---

## üìû Fragen vor jedem Review

Stelle Dir diese Fragen vor dem Absenden:

1. Habe ich den Build ausgef√ºhrt? **JA / NEIN**
2. Habe ich jede "fehlt"-Behauptung mit `find` gepr√ºft? **JA / NEIN**
3. Hat jedes Problem einen Dateipfad? **JA / NEIN**
4. Ist jedes Problem reproduzierbar? **JA / NEIN**
5. Sind meine Priorit√§ten korrekt (P0 nur f√ºr Build-Breaker)? **JA / NEIN**

**Wenn auch nur eine Antwort "NEIN" ist: Review √ºberarbeiten!**

---

**Erstellt:** 2025-10-02
**Grund:** Fehlerhafter Review ohne Build-Verifikation
**Autor:** Dokumentiert nach Feedback von Torsten Liermann
**Status:** VERBINDLICH f√ºr alle zuk√ºnftigen Reviews
